{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48060c32",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Minesweeper LLM Competition - Custom GRPO Training\n",
    "\n",
    "## Goal\n",
    "Finetune an LLM with LoRA using GRPO to play Minesweeper by:\n",
    "- **Input**: JSON game state (board configuration)\n",
    "- **Output**: JSON action (reveal or flag a cell)\n",
    "\n",
    "Teams will compete to train the best Minesweeper-playing LLM!\n",
    "\n",
    "## Training Approach\n",
    "- **Model**: Qwen2.5-14B-Instruct (from /root/.cache/huggingface/hub)\n",
    "- **Method**: GRPO (Group Relative Policy Optimization)\n",
    "- **Framework**: Unsloth (2-6x faster, 70% less VRAM)\n",
    "- **Hardware**: AMD MI300X GPU (192GB HBM3, ROCm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5c2e56",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Load Model with Unsloth\n",
    "\n",
    "Load Qwen3-4B with LoRA configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35e1118e-9532-4aa3-a4eb-ecf1bb2abb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"./workspace/hf_cache\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"./workspace/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"./workspace/hf_cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"./workspace/hf_cache\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11ecb51d-67e6-42e4-b739-cea6e57ab2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignored error while writing commit hash to /root/.cache/huggingface/models--unsloth--Qwen2.5-14B-Instruct/refs/main: [Errno 30] Read-only file system: '/root/.cache/huggingface/models--unsloth--Qwen2.5-14B-Instruct'.\n",
      "[2026-02-14 12:21:41] WARNING _snapshot_download.py:300: Ignored error while writing commit hash to /root/.cache/huggingface/models--unsloth--Qwen2.5-14B-Instruct/refs/main: [Errno 30] Read-only file system: '/root/.cache/huggingface/models--unsloth--Qwen2.5-14B-Instruct'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/workspace/workspace/Qwen2.5-14B-Instruct'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"unsloth/Qwen2.5-14B-Instruct\",\n",
    "    local_dir=\"./workspace/Qwen2.5-14B-Instruct\",\n",
    "    local_dir_use_symlinks=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8af32493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "==((====))==  Unsloth 2025.10.6: Fast Qwen2 patching. Transformers: 4.56.2. vLLM: 0.11.1rc2.dev161+g8a297115e.rocm700.\n",
      "   \\\\   /|    . Num GPUs = 1. Max memory: 255.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+gitb2fb688. ROCm Toolkit: 7.0.51831-a3e329ad8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02af46566cd4473087c192a21fb727c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"/workspace/workspace/Qwen2.5-14B-Instruct\",\n",
    "    load_in_4bit=False,   # AMD → 4bit disabled\n",
    "    max_seq_length=1024,\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(\"Device:\", model.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9f0712",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Add LoRA Adapters\n",
    "\n",
    "Add LoRA layers for efficient finetuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd54ba09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.10.6 patched 48 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA config: rank=32, alpha=32, dropout=0.05\n",
      "trainable params: 137,625,600 || all params: 14,907,659,264 || trainable%: 0.9232\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank,           # alpha = rank → scaling factor = 1.0 (stable training)\n",
    "    lora_dropout = 0.05,              # Small dropout for regularization\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")\n",
    "print(f\"LoRA config: rank={lora_rank}, alpha={lora_rank}, dropout=0.05\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4503f9af",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Minesweeper Game Implementation\n",
    "\n",
    "Custom Minesweeper environment supporting:\n",
    "- Customizable board size and mine count\n",
    "- Actions: reveal or flag cells\n",
    "- Win: reveal all safe cells\n",
    "- Lose: reveal a mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5fc3220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple, Optional, Set\n",
    "import random\n",
    "\n",
    "@dataclass\n",
    "class MinesweeperGame:\n",
    "    rows: int\n",
    "    cols: int\n",
    "    num_mines: int\n",
    "    seed: Optional[int] = None\n",
    "    _rng: random.Random = field(init=False, repr=False)\n",
    "    _board: List[List[int]] = field(init=False, repr=False)  # -1 = mine, 0-8 = count\n",
    "    _revealed: Set[Tuple[int, int]] = field(init=False, repr=False, default_factory=set)\n",
    "    _flagged: Set[Tuple[int, int]] = field(init=False, repr=False, default_factory=set)\n",
    "    _state: str = field(default=\"ongoing\", init=False, repr=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.num_mines >= self.rows * self.cols:\n",
    "            raise ValueError(\"Too many mines for board size\")\n",
    "        self._rng = random.Random(self.seed)\n",
    "        self._board = [[0 for _ in range(self.cols)] for _ in range(self.rows)]\n",
    "        self._place_mines()\n",
    "        self._calculate_numbers()\n",
    "\n",
    "    def _place_mines(self):\n",
    "        \"\"\"Place mines randomly on the board\"\"\"\n",
    "        positions = [(r, c) for r in range(self.rows) for c in range(self.cols)]\n",
    "        mine_positions = self._rng.sample(positions, self.num_mines)\n",
    "        for r, c in mine_positions:\n",
    "            self._board[r][c] = -1\n",
    "\n",
    "    def _calculate_numbers(self):\n",
    "        \"\"\"Calculate numbers for each cell based on adjacent mines\"\"\"\n",
    "        for r in range(self.rows):\n",
    "            for c in range(self.cols):\n",
    "                if self._board[r][c] == -1:\n",
    "                    continue\n",
    "                count = 0\n",
    "                for dr in [-1, 0, 1]:\n",
    "                    for dc in [-1, 0, 1]:\n",
    "                        if dr == 0 and dc == 0:\n",
    "                            continue\n",
    "                        nr, nc = r + dr, c + dc\n",
    "                        if 0 <= nr < self.rows and 0 <= nc < self.cols:\n",
    "                            if self._board[nr][nc] == -1:\n",
    "                                count += 1\n",
    "                self._board[r][c] = count\n",
    "\n",
    "    def _reveal_cell(self, row: int, col: int) -> bool:\n",
    "        \"\"\"Reveal a cell. Returns True if valid move, False if invalid.\n",
    "        Uses iterative flood-fill to avoid recursion limit on large boards.\n",
    "        (Issue #11: was recursive; Issue typo: fixed 'bself' -> 'self')\n",
    "        \"\"\"\n",
    "        if not (0 <= row < self.rows and 0 <= col < self.cols):\n",
    "            return False\n",
    "        if (row, col) in self._revealed or (row, col) in self._flagged:\n",
    "            return False\n",
    "\n",
    "        stack = [(row, col)]\n",
    "        while stack:\n",
    "            r, c = stack.pop()\n",
    "            if (r, c) in self._revealed:\n",
    "                continue\n",
    "\n",
    "            self._revealed.add((r, c))\n",
    "\n",
    "            # Hit a mine!\n",
    "            if self._board[r][c] == -1:\n",
    "                self._state = \"failed\"\n",
    "                return True\n",
    "\n",
    "            # Auto-reveal neighbors if cell is 0\n",
    "            if self._board[r][c] == 0:\n",
    "                for dr in [-1, 0, 1]:\n",
    "                    for dc in [-1, 0, 1]:\n",
    "                        if dr == 0 and dc == 0:\n",
    "                            continue\n",
    "                        nr, nc = r + dr, c + dc\n",
    "                        if (0 <= nr < self.rows and 0 <= nc < self.cols\n",
    "                                and (nr, nc) not in self._revealed\n",
    "                                and (nr, nc) not in self._flagged):\n",
    "                            stack.append((nr, nc))\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _flag_cell(self, row: int, col: int) -> bool:\n",
    "        \"\"\"Flag/unflag a cell. Returns True if valid, False if invalid\"\"\"\n",
    "        if not (0 <= row < self.rows and 0 <= col < self.cols):\n",
    "            return False\n",
    "        if (row, col) in self._revealed:\n",
    "            return False\n",
    "        \n",
    "        if (row, col) in self._flagged:\n",
    "            self._flagged.remove((row, col))\n",
    "        else:\n",
    "            self._flagged.add((row, col))\n",
    "        return True\n",
    "\n",
    "    def do_action(self, action: dict) -> str:\n",
    "        \"\"\"Execute an action and return a status string.\n",
    "\n",
    "        Returns one of:\n",
    "          'ok'               - valid move executed\n",
    "          'mine'             - revealed a mine (game over)\n",
    "          'win'              - game won after this move\n",
    "          'invalid_format'   - bad action dict / missing keys / bad types\n",
    "          'out_of_bounds'    - coordinates outside the board\n",
    "          'already_revealed' - cell was already revealed\n",
    "          'flagged_cell'     - tried to reveal a flagged cell\n",
    "          'invalid_flag'     - tried to flag a revealed cell\n",
    "          'game_over'        - game was already over before this call\n",
    "\n",
    "        (Issue #13: previously set state='failed' for ALL invalid moves,\n",
    "         conflating formatting errors with hitting a mine.)\n",
    "        \"\"\"\n",
    "        if self._state != \"ongoing\":\n",
    "            return \"game_over\"\n",
    "\n",
    "        if not isinstance(action, dict):\n",
    "            self._state = \"failed\"\n",
    "            return \"invalid_format\"\n",
    "\n",
    "        action_type = action.get(\"type\")\n",
    "        row = action.get(\"row\")\n",
    "        col = action.get(\"col\")\n",
    "\n",
    "        if action_type not in [\"reveal\", \"flag\"] or row is None or col is None:\n",
    "            self._state = \"failed\"\n",
    "            return \"invalid_format\"\n",
    "\n",
    "        try:\n",
    "            row, col = int(row), int(col)\n",
    "        except (ValueError, TypeError):\n",
    "            self._state = \"failed\"\n",
    "            return \"invalid_format\"\n",
    "\n",
    "        if not (0 <= row < self.rows and 0 <= col < self.cols):\n",
    "            self._state = \"failed\"\n",
    "            return \"out_of_bounds\"\n",
    "\n",
    "        if action_type == \"reveal\":\n",
    "            if (row, col) in self._revealed:\n",
    "                self._state = \"failed\"\n",
    "                return \"already_revealed\"\n",
    "            if (row, col) in self._flagged:\n",
    "                self._state = \"failed\"\n",
    "                return \"flagged_cell\"\n",
    "            valid = self._reveal_cell(row, col)\n",
    "        else:\n",
    "            if (row, col) in self._revealed:\n",
    "                self._state = \"failed\"\n",
    "                return \"invalid_flag\"\n",
    "            valid = self._flag_cell(row, col)\n",
    "\n",
    "        if not valid:\n",
    "            self._state = \"failed\"\n",
    "            return \"invalid_format\"\n",
    "\n",
    "        self._check_win()\n",
    "\n",
    "        if self._state == \"failed\":\n",
    "            return \"mine\"\n",
    "        if self._state == \"success\":\n",
    "            return \"win\"\n",
    "        return \"ok\"\n",
    "\n",
    "    def _check_win(self):\n",
    "        \"\"\"Check if player has won\"\"\"\n",
    "        total_cells = self.rows * self.cols\n",
    "        safe_cells = total_cells - self.num_mines\n",
    "        if len(self._revealed) == safe_cells:\n",
    "            self._state = \"success\"\n",
    "\n",
    "    def get_visible_board(self) -> List[List[str]]:\n",
    "        \"\"\"Get board state as player sees it\"\"\"\n",
    "        visible = []\n",
    "        for r in range(self.rows):\n",
    "            row = []\n",
    "            for c in range(self.cols):\n",
    "                if (r, c) in self._flagged:\n",
    "                    row.append('F')\n",
    "                elif (r, c) in self._revealed:\n",
    "                    val = self._board[r][c]\n",
    "                    row.append('*' if val == -1 else str(val))\n",
    "                else:\n",
    "                    row.append('.')\n",
    "            visible.append(row)\n",
    "        return visible\n",
    "\n",
    "    def state(self) -> str:\n",
    "        return self._state\n",
    "\n",
    "    def pretty_print(self) -> str:\n",
    "        \"\"\"Pretty print the board\"\"\"\n",
    "        visible = self.get_visible_board()\n",
    "        lines = []\n",
    "        \n",
    "        # Header\n",
    "        header = \"   \" + \" \".join(f\"{i:2d}\" for i in range(self.cols))\n",
    "        lines.append(header)\n",
    "        lines.append(\"  \" + \"─\" * (self.cols * 3 + 1))\n",
    "        \n",
    "        # Board\n",
    "        for r, row in enumerate(visible):\n",
    "            line = f\"{r:2d}│ \" + \"  \".join(row)\n",
    "            lines.append(line)\n",
    "        \n",
    "        return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b617e14",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# JSON Input/Output Format\n",
    "\n",
    "## Input Format (Game State)\n",
    "```json\n",
    "{\n",
    "  \"board\": [\n",
    "    [\"1\", \".\", \".\", \".\", \".\", \".\"],\n",
    "    [\".\", \".\", \".\", \".\", \".\", \".\"],\n",
    "    [\".\", \".\", \".\", \".\", \".\", \".\"],\n",
    "    [\".\", \".\", \".\", \".\", \".\", \".\"],\n",
    "    [\".\", \".\", \".\", \".\", \".\", \".\"],\n",
    "    [\".\", \".\", \".\", \".\", \".\", \".\"]\n",
    "  ],\n",
    "  \"rows\": 6,\n",
    "  \"cols\": 6,\n",
    "  \"mines\": 5,\n",
    "  \"flags_placed\": 0,\n",
    "  \"cells_revealed\": 0\n",
    "}\n",
    "```\n",
    "\n",
    "## Output Format (Action)\n",
    "```json\n",
    "{\"type\": \"reveal\", \"row\": 2, \"col\": 3}\n",
    "```\n",
    "or\n",
    "```json\n",
    "{\"type\": \"flag\", \"row\": 1, \"col\": 4}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae25ecd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert Minesweeper solver. Output ONE action as JSON only.\n",
      "\n",
      "RULES:\n",
      "- Numbers show how many of their 8 neighbors are mines.\n",
      "- Subtract flagged neighbors from the number to find remaining mines.\n",
      "- If remaining mines == remaining unrevealed neighbors → all are mines → FLAG.\n",
      "- If remaining mines == 0 → all unrevealed neighbors are safe → REVEAL.\n",
      "- Never reveal a flagged cell or flag a revealed cell.\n",
      "- Prefer logically deducible moves over guessing.\n",
      "\n",
      "{\n",
      "  \"board\": [\n",
      "    [\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\"\n",
      "    ],\n",
      "    [\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\"\n",
      "    ],\n",
      "    [\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\"\n",
      "    ],\n",
      "    [\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\"\n",
      "    ],\n",
      "    [\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\"\n",
      "    ],\n",
      "    [\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\",\n",
      "      \".\"\n",
      "    ]\n",
      "  ],\n",
      "  \"rows\": 6,\n",
      "  \"cols\": 6,\n",
      "  \"mines\": 5,\n",
      "  \"flags_placed\": 0,\n",
      "  \"cells_revealed\": 0,\n",
      "  \"remaining_mines\": 5\n",
      "}\n",
      "\n",
      "\".\"=unrevealed \"F\"=flagged \"0\"-\"8\"=adjacent mine count\n",
      "\n",
      "No cells can be logically deduced — pick the least risky unrevealed cell.\n",
      "\n",
      "Respond ONLY with a JSON object:\n",
      "{\"type\":\"reveal\"|\"flag\",\"row\":<int>,\"col\":<int>}\n",
      "\n",
      "--- Prompt length: 1270 chars ---\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# Minesweeper Logic Helpers — used by prompt AND reward functions\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def _compute_safe_cells(game: MinesweeperGame) -> list:\n",
    "    \"\"\"Find cells that are logically guaranteed safe.\n",
    "    A cell is safe if any adjacent revealed number already has all its\n",
    "    mines accounted for by flags (remaining_mines == 0).\"\"\"\n",
    "    safe = set()\n",
    "    for r in range(game.rows):\n",
    "        for c in range(game.cols):\n",
    "            if (r, c) not in game._revealed:\n",
    "                continue\n",
    "            val = game._board[r][c]\n",
    "            if val <= 0:\n",
    "                continue\n",
    "            flags = 0\n",
    "            unrevealed = []\n",
    "            for dr in [-1, 0, 1]:\n",
    "                for dc in [-1, 0, 1]:\n",
    "                    if dr == 0 and dc == 0:\n",
    "                        continue\n",
    "                    nr, nc = r + dr, c + dc\n",
    "                    if 0 <= nr < game.rows and 0 <= nc < game.cols:\n",
    "                        if (nr, nc) in game._flagged:\n",
    "                            flags += 1\n",
    "                        elif (nr, nc) not in game._revealed:\n",
    "                            unrevealed.append((nr, nc))\n",
    "            if val - flags == 0:\n",
    "                for cell in unrevealed:\n",
    "                    safe.add(cell)\n",
    "    return [list(c) for c in safe]\n",
    "\n",
    "\n",
    "def _compute_mine_cells(game: MinesweeperGame) -> list:\n",
    "    \"\"\"Find cells that are logically guaranteed mines.\n",
    "    A cell is a mine if an adjacent number has remaining_mines ==\n",
    "    remaining unrevealed neighbors.\"\"\"\n",
    "    mines = set()\n",
    "    for r in range(game.rows):\n",
    "        for c in range(game.cols):\n",
    "            if (r, c) not in game._revealed:\n",
    "                continue\n",
    "            val = game._board[r][c]\n",
    "            if val <= 0:\n",
    "                continue\n",
    "            flags = 0\n",
    "            unrevealed = []\n",
    "            for dr in [-1, 0, 1]:\n",
    "                for dc in [-1, 0, 1]:\n",
    "                    if dr == 0 and dc == 0:\n",
    "                        continue\n",
    "                    nr, nc = r + dr, c + dc\n",
    "                    if 0 <= nr < game.rows and 0 <= nc < game.cols:\n",
    "                        if (nr, nc) in game._flagged:\n",
    "                            flags += 1\n",
    "                        elif (nr, nc) not in game._revealed:\n",
    "                            unrevealed.append((nr, nc))\n",
    "            remaining = val - flags\n",
    "            if remaining > 0 and remaining == len(unrevealed):\n",
    "                for cell in unrevealed:\n",
    "                    mines.add(cell)\n",
    "    return [list(c) for c in mines]\n",
    "\n",
    "\n",
    "def _is_logically_safe(game: MinesweeperGame, row: int, col: int) -> bool:\n",
    "    \"\"\"Check if (row, col) appears in the set of logically safe cells.\"\"\"\n",
    "    return [row, col] in _compute_safe_cells(game)\n",
    "\n",
    "\n",
    "def _is_logically_mine(game: MinesweeperGame, row: int, col: int) -> bool:\n",
    "    \"\"\"Check if (row, col) appears in the set of logically certain mines.\"\"\"\n",
    "    return [row, col] in _compute_mine_cells(game)\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# Prompt formatting — concise, hint-enriched, JSON-only output\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def format_state_for_llm(game: MinesweeperGame) -> str:\n",
    "    \"\"\"Convert game state to an optimized prompt for LLM.\n",
    "\n",
    "    Key design choices (backed by research):\n",
    "    - Provide pre-computed logical hints so the model doesn't need full\n",
    "      constraint-satisfaction from scratch.\n",
    "    - Keep instructions terse → less chance of exceeding max_completion_length.\n",
    "    - Explicitly forbid explanation text → pure JSON output.\n",
    "    - Include remaining_mines count to guide flagging strategy.\n",
    "    \"\"\"\n",
    "    board = game.get_visible_board()\n",
    "    state = {\n",
    "        \"board\": board,\n",
    "        \"rows\": game.rows,\n",
    "        \"cols\": game.cols,\n",
    "        \"mines\": game.num_mines,\n",
    "        \"flags_placed\": len(game._flagged),\n",
    "        \"cells_revealed\": len(game._revealed),\n",
    "        \"remaining_mines\": game.num_mines - len(game._flagged),\n",
    "    }\n",
    "\n",
    "    # Pre-compute logical deductions as hints\n",
    "    safe_cells = _compute_safe_cells(game)\n",
    "    mine_cells = _compute_mine_cells(game)\n",
    "\n",
    "    hint_lines = []\n",
    "    if safe_cells:\n",
    "        hint_lines.append(f\"Logically SAFE cells (reveal one): {safe_cells[:6]}\")\n",
    "    if mine_cells:\n",
    "        hint_lines.append(f\"Logically CERTAIN mines (flag one): {mine_cells[:6]}\")\n",
    "    if not safe_cells and not mine_cells:\n",
    "        hint_lines.append(\"No cells can be logically deduced — pick the least risky unrevealed cell.\")\n",
    "\n",
    "    hint_section = \"\\n\".join(hint_lines)\n",
    "\n",
    "    prompt = f\"\"\"You are an expert Minesweeper solver. Output ONE action as JSON only.\n",
    "\n",
    "RULES:\n",
    "- Numbers show how many of their 8 neighbors are mines.\n",
    "- Subtract flagged neighbors from the number to find remaining mines.\n",
    "- If remaining mines == remaining unrevealed neighbors → all are mines → FLAG.\n",
    "- If remaining mines == 0 → all unrevealed neighbors are safe → REVEAL.\n",
    "- Never reveal a flagged cell or flag a revealed cell.\n",
    "- Prefer logically deducible moves over guessing.\n",
    "\n",
    "{json.dumps(state, indent=2)}\n",
    "\n",
    "\".\"=unrevealed \"F\"=flagged \"0\"-\"8\"=adjacent mine count\n",
    "\n",
    "{hint_section}\n",
    "\n",
    "Respond ONLY with a JSON object:\n",
    "{{\"type\":\"reveal\"|\"flag\",\"row\":<int>,\"col\":<int>}}\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def parse_llm_action(response: str) -> dict:\n",
    "    \"\"\"Extract JSON action from LLM response.\n",
    "\n",
    "    Finds all JSON-like objects and returns the LAST one matching the\n",
    "    expected schema. LLMs typically place their final answer at the end.\n",
    "    \"\"\"\n",
    "    best = None\n",
    "    for match in re.finditer(r'\\{[^{}]*\\}', response):\n",
    "        try:\n",
    "            action = json.loads(match.group())\n",
    "            if (\"type\" in action and \"row\" in action and \"col\" in action\n",
    "                    and action[\"type\"] in [\"reveal\", \"flag\"]):\n",
    "                best = action\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    return best\n",
    "\n",
    "# ── Quick test ──\n",
    "game = MinesweeperGame(rows=6, cols=6, num_mines=5)\n",
    "prompt = format_state_for_llm(game)\n",
    "print(prompt)\n",
    "print(f\"\\n--- Prompt length: {len(prompt)} chars ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5302a238",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Test Model Before Training\n",
    "\n",
    "See how the base model performs without finetuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6fd8563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Base Model Response ===\n",
      "{\"type\":\"reveal\",\"row\":0,\"col\":0}<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "game = MinesweeperGame(rows=6, cols=6, num_mines=5, seed=42)\n",
    "prompt = format_state_for_llm(game)\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": prompt}],\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True,\n",
    ")\n",
    "\n",
    "print(\"=== Base Model Response ===\")\n",
    "output = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.9,\n",
    "    max_new_tokens = 128,\n",
    "    do_sample = True,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444b3c5f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# GRPO Reward Functions\n",
    "\n",
    "Define reward functions to guide the model's learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b64ca52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All reward functions defined:\n",
      "   1. valid_json_reward   — format + conciseness\n",
      "   2. gameplay_scores     — all 12 criteria\n",
      "   3. strategic_reward    — logical deduction bonuses\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# Reward 1: Valid JSON format\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def valid_json_reward(completions, **kwargs):\n",
    "    \"\"\"Reward valid JSON action format. Also rewards conciseness.\"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"].strip()\n",
    "        action = parse_llm_action(response)\n",
    "\n",
    "        if action is None:\n",
    "            scores.append(-5.0)  # Invalid format\n",
    "            continue\n",
    "\n",
    "        # Bonus for pure JSON (no extra text)\n",
    "        try:\n",
    "            parsed = json.loads(response)\n",
    "            if \"type\" in parsed and \"row\" in parsed and \"col\" in parsed:\n",
    "                scores.append(3.0)  # Perfect — pure JSON only\n",
    "                continue\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "        # Valid JSON but with extra surrounding text\n",
    "        json_match = re.search(r'\\{[^{}]*\\}', response)\n",
    "        extra_chars = len(response) - len(json_match.group()) if json_match else len(response)\n",
    "        if extra_chars < 10:\n",
    "            scores.append(2.0)\n",
    "        elif extra_chars < 50:\n",
    "            scores.append(1.0)\n",
    "        elif extra_chars < 200:\n",
    "            scores.append(-0.5)\n",
    "        else:\n",
    "            scores.append(-2.0)  # Way too verbose\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# Reward 2: Gameplay — all 12 scoring criteria implemented\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def gameplay_scores(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Complete gameplay reward implementing all 12 scoring criteria:\n",
    "\n",
    "    1.  Flag cell that IS a mine        → +15\n",
    "    2.  Flag cell that is NOT a mine    → -10\n",
    "    3.  Reveal cell that IS a mine      → -25\n",
    "    4.  Reveal cell that is safe        → +10 (guess) / +15 (logically deducible)\n",
    "    5.  Flag already flagged cell       → -12\n",
    "    6.  Reveal already revealed cell    → -12\n",
    "    7.  Out of bounds                   → -15\n",
    "    8.  Total flags > total mines       → -10 (additional)\n",
    "    9.  Invalid JSON                    → -50\n",
    "    10. Win the game                    → +100\n",
    "    11. Reveal a flagged cell           → -8\n",
    "    12. Flag a revealed cell            → -8\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "\n",
    "    seeds = kwargs.get(\"seed\", [])\n",
    "    move_histories = kwargs.get(\"move_history\", [])\n",
    "\n",
    "    for idx, completion in enumerate(completions):\n",
    "        response = completion[0][\"content\"]\n",
    "        action = parse_llm_action(response)\n",
    "\n",
    "        # ── Criterion 9: Invalid JSON ──\n",
    "        if action is None:\n",
    "            scores.append(-50.0)\n",
    "            continue\n",
    "\n",
    "        # ── Reconstruct game state ──\n",
    "        if idx >= len(seeds) or idx >= len(move_histories):\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "\n",
    "        seed = seeds[idx]\n",
    "        move_history_raw = move_histories[idx]\n",
    "        if isinstance(move_history_raw, str):\n",
    "            move_history = json.loads(move_history_raw)\n",
    "        else:\n",
    "            move_history = move_history_raw\n",
    "\n",
    "        game = MinesweeperGame(rows=6, cols=6, num_mines=5, seed=seed)\n",
    "        for prev_action in move_history:\n",
    "            game.do_action(prev_action)\n",
    "\n",
    "        row, col = action[\"row\"], action[\"col\"]\n",
    "        action_type = action[\"type\"]\n",
    "\n",
    "        # ── Criterion 7: Out of bounds ──\n",
    "        if not (0 <= row < game.rows and 0 <= col < game.cols):\n",
    "            scores.append(-15.0)\n",
    "            continue\n",
    "\n",
    "        score = 0.0\n",
    "\n",
    "        if action_type == \"reveal\":\n",
    "            # ── Criterion 6: Reveal already revealed cell ──\n",
    "            if (row, col) in game._revealed:\n",
    "                scores.append(-12.0)\n",
    "                continue\n",
    "\n",
    "            # ── Criterion 11: Reveal a flagged cell ──\n",
    "            if (row, col) in game._flagged:\n",
    "                scores.append(-8.0)\n",
    "                continue\n",
    "\n",
    "            # ── Criterion 3: Reveal a mine ──\n",
    "            if game._board[row][col] == -1:\n",
    "                scores.append(-25.0)\n",
    "                continue\n",
    "\n",
    "            # ── Criterion 4: Reveal safe cell ──\n",
    "            if _is_logically_safe(game, row, col):\n",
    "                score += 15.0   # Logically deduced safe cell\n",
    "            else:\n",
    "                score += 10.0   # Random / guessed safe cell\n",
    "\n",
    "            # Small bonus for revealing cells adjacent to numbers (information-rich)\n",
    "            board = game.get_visible_board()\n",
    "            for dr in [-1, 0, 1]:\n",
    "                for dc in [-1, 0, 1]:\n",
    "                    nr, nc = row + dr, col + dc\n",
    "                    if 0 <= nr < game.rows and 0 <= nc < game.cols:\n",
    "                        if board[nr][nc] in ['1', '2', '3', '4', '5', '6', '7', '8']:\n",
    "                            score += 1.0\n",
    "                            break\n",
    "                else:\n",
    "                    continue\n",
    "                break\n",
    "\n",
    "            # ── Criterion 10: Check for win after this reveal ──\n",
    "            game_copy = MinesweeperGame(rows=6, cols=6, num_mines=5, seed=seed)\n",
    "            for prev_action in move_history:\n",
    "                game_copy.do_action(prev_action)\n",
    "            game_copy.do_action(action)\n",
    "            if game_copy.state() == \"success\":\n",
    "                score += 100.0\n",
    "\n",
    "        elif action_type == \"flag\":\n",
    "            # ── Criterion 5: Flag already flagged cell ──\n",
    "            if (row, col) in game._flagged:\n",
    "                scores.append(-12.0)\n",
    "                continue\n",
    "\n",
    "            # ── Criterion 12: Flag a revealed cell ──\n",
    "            if (row, col) in game._revealed:\n",
    "                scores.append(-8.0)\n",
    "                continue\n",
    "\n",
    "            # ── Criterion 1: Flag a mine (correct) ──\n",
    "            if game._board[row][col] == -1:\n",
    "                if _is_logically_mine(game, row, col):\n",
    "                    score += 20.0   # Logically deduced mine\n",
    "                else:\n",
    "                    score += 15.0   # Correct but guessed\n",
    "\n",
    "            # ── Criterion 2: Flag a non-mine (wrong) ──\n",
    "            else:\n",
    "                score += -10.0\n",
    "\n",
    "            # ── Criterion 8: Total flags > total mines ──\n",
    "            new_flag_count = len(game._flagged) + 1\n",
    "            if new_flag_count > game.num_mines:\n",
    "                score -= 10.0\n",
    "\n",
    "            # ── Criterion 10: Check for win after this flag ──\n",
    "            game_copy = MinesweeperGame(rows=6, cols=6, num_mines=5, seed=seed)\n",
    "            for prev_action in move_history:\n",
    "                game_copy.do_action(prev_action)\n",
    "            game_copy.do_action(action)\n",
    "            if game_copy.state() == \"success\":\n",
    "                score += 100.0\n",
    "\n",
    "        else:\n",
    "            scores.append(-10.0)\n",
    "            continue\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "# Reward 3: Strategic play — rewards logical deduction over guessing\n",
    "# ──────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def strategic_reward(completions, **kwargs):\n",
    "    \"\"\"Reward strategic play patterns:\n",
    "    - Choosing logically deducible moves when available\n",
    "    - Opening in corners/edges (lower mine probability on fresh boards)\n",
    "    - Penalize ignoring available deductions\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    seeds = kwargs.get(\"seed\", [])\n",
    "    move_histories = kwargs.get(\"move_history\", [])\n",
    "\n",
    "    for idx, completion in enumerate(completions):\n",
    "        response = completion[0][\"content\"]\n",
    "        action = parse_llm_action(response)\n",
    "\n",
    "        if action is None:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "\n",
    "        if idx >= len(seeds) or idx >= len(move_histories):\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "\n",
    "        seed = seeds[idx]\n",
    "        mh_raw = move_histories[idx]\n",
    "        move_history = json.loads(mh_raw) if isinstance(mh_raw, str) else mh_raw\n",
    "\n",
    "        game = MinesweeperGame(rows=6, cols=6, num_mines=5, seed=seed)\n",
    "        for prev in move_history:\n",
    "            game.do_action(prev)\n",
    "\n",
    "        row, col = action[\"row\"], action[\"col\"]\n",
    "        action_type = action[\"type\"]\n",
    "        score = 0.0\n",
    "\n",
    "        if not (0 <= row < game.rows and 0 <= col < game.cols):\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # ── Fresh game opening strategy ──\n",
    "        if len(game._revealed) == 0 and action_type == \"reveal\":\n",
    "            corners = [(0, 0), (0, game.cols - 1),\n",
    "                       (game.rows - 1, 0), (game.rows - 1, game.cols - 1)]\n",
    "            if (row, col) in corners:\n",
    "                score += 2.0   # Corners have only 3 neighbors → safer\n",
    "\n",
    "        # ── Reward choosing logically deducible moves ──\n",
    "        safe_cells = _compute_safe_cells(game)\n",
    "        mine_cells = _compute_mine_cells(game)\n",
    "\n",
    "        if action_type == \"reveal\" and [row, col] in safe_cells:\n",
    "            score += 5.0   # Chose a provably safe cell\n",
    "        elif action_type == \"flag\" and [row, col] in mine_cells:\n",
    "            score += 5.0   # Chose a provably mine cell\n",
    "        elif safe_cells or mine_cells:\n",
    "            # Deducible moves existed but agent didn't pick one\n",
    "            score -= 3.0\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "print(\"✅ All reward functions defined:\")\n",
    "print(\"   1. valid_json_reward   — format + conciseness\")\n",
    "print(\"   2. gameplay_scores     — all 12 criteria\")\n",
    "print(\"   3. strategic_reward    — logical deduction bonuses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b787f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Create Training Dataset\n",
    "\n",
    "Generate diverse game states for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "993c0d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training dataset with curriculum learning...\n",
      "Created 2000 training examples (all ongoing games)\n",
      "\n",
      "  Fresh games (0 moves): 637 (31.9%)\n",
      "  Early game (1-2):      833 (41.6%)\n",
      "  Mid game (3-8):        516 (25.8%)\n",
      "  Late game (9+):        14 (0.7%)\n",
      "  Avg moves per state:   1.9\n",
      "\n",
      "Example training prompt (first 300 chars):\n",
      "You are an expert Minesweeper solver. Output ONE action as JSON only.\n",
      "\n",
      "RULES:\n",
      "- Numbers show how many of their 8 neighbors are mines.\n",
      "- Subtract flagged neighbors from the number to find remaining mines.\n",
      "- If remaining mines == remaining unrevealed neighbors → all are mines → FLAG.\n",
      "- If remaining mi...\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def generate_game_states(num_samples=2000, rows=6, cols=6, num_mines=5,\n",
    "                         rng_seed=42):\n",
    "    \"\"\"\n",
    "    Generate diverse Minesweeper game states with CURRICULUM LEARNING.\n",
    "\n",
    "    Distribution by game phase (backed by Bengio et al. 2009):\n",
    "    - 15% fresh games (0 moves)   → learn opening strategy\n",
    "    - 25% early game  (1-2 moves) → learn basic deduction\n",
    "    - 40% mid game    (3-8 moves) → learn complex constraint satisfaction\n",
    "    - 20% late game   (9+ moves)  → learn endgame / flagging\n",
    "\n",
    "    Also includes flag-in-progress states (10%) so the model\n",
    "    learns when/how to flag.\n",
    "\n",
    "    IMPORTANTLY: Stores seed + move_history (as JSON string) so reward\n",
    "    function can reconstruct the EXACT game state.\n",
    "    \"\"\"\n",
    "    np.random.seed(rng_seed)\n",
    "    random.seed(rng_seed)\n",
    "\n",
    "    dataset_items = []\n",
    "    attempts = 0\n",
    "    max_attempts = num_samples * 5\n",
    "\n",
    "    # Move-count distribution for curriculum\n",
    "    move_bins = [\n",
    "        (0, 0, 0.15),    # Fresh\n",
    "        (1, 2, 0.25),    # Early\n",
    "        (3, 8, 0.40),    # Mid\n",
    "        (9, 20, 0.20),   # Late\n",
    "    ]\n",
    "\n",
    "    while len(dataset_items) < num_samples and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "\n",
    "        # Sample which phase\n",
    "        phase_rand = np.random.random()\n",
    "        cumulative = 0\n",
    "        min_moves, max_moves_range = 0, 0\n",
    "        for mn, mx, prob in move_bins:\n",
    "            cumulative += prob\n",
    "            if phase_rand < cumulative:\n",
    "                min_moves, max_moves_range = mn, mx\n",
    "                break\n",
    "        num_moves = np.random.randint(min_moves, max_moves_range + 1)\n",
    "\n",
    "        seed = np.random.randint(100000)\n",
    "        game = MinesweeperGame(rows=rows, cols=cols, num_mines=num_mines, seed=seed)\n",
    "        move_history = []\n",
    "\n",
    "        # Occasionally include flag actions in history (10% chance per move)\n",
    "        for _ in range(num_moves):\n",
    "            board = game.get_visible_board()\n",
    "            unrevealed = [(r, c) for r in range(rows) for c in range(cols)\n",
    "                         if board[r][c] == '.']\n",
    "\n",
    "            if not unrevealed or game.state() != \"ongoing\":\n",
    "                break\n",
    "\n",
    "            # 10% chance to flag instead of reveal (to train flag awareness)\n",
    "            if np.random.random() < 0.10 and len(game._flagged) < num_mines:\n",
    "                r, c = random.choice(unrevealed)\n",
    "                action = {\"type\": \"flag\", \"row\": r, \"col\": c}\n",
    "            else:\n",
    "                r, c = random.choice(unrevealed)\n",
    "                action = {\"type\": \"reveal\", \"row\": r, \"col\": c}\n",
    "\n",
    "            game.do_action(action)\n",
    "            move_history.append(action)\n",
    "\n",
    "        # Only add ongoing games\n",
    "        if game.state() == \"ongoing\":\n",
    "            prompt_text = format_state_for_llm(game)\n",
    "            dataset_items.append({\n",
    "                \"prompt\": [{\"role\": \"user\", \"content\": prompt_text}],\n",
    "                \"seed\": seed,\n",
    "                \"move_history\": json.dumps(move_history),\n",
    "            })\n",
    "\n",
    "    dataset_items = dataset_items[:num_samples]\n",
    "    return Dataset.from_list(dataset_items)\n",
    "\n",
    "# Generate training dataset\n",
    "print(\"Generating training dataset with curriculum learning...\")\n",
    "dataset = generate_game_states(num_samples=2000, rows=6, cols=6, num_mines=5)\n",
    "print(f\"Created {len(dataset)} training examples (all ongoing games)\")\n",
    "\n",
    "# Distribution analysis\n",
    "fresh_count = sum(1 for item in dataset if item[\"move_history\"] == \"[]\")\n",
    "move_counts = [len(json.loads(item[\"move_history\"])) for item in dataset]\n",
    "print(f\"\\n  Fresh games (0 moves): {fresh_count} ({fresh_count/len(dataset)*100:.1f}%)\")\n",
    "print(f\"  Early game (1-2):      {sum(1 for m in move_counts if 1 <= m <= 2)} ({sum(1 for m in move_counts if 1 <= m <= 2)/len(dataset)*100:.1f}%)\")\n",
    "print(f\"  Mid game (3-8):        {sum(1 for m in move_counts if 3 <= m <= 8)} ({sum(1 for m in move_counts if 3 <= m <= 8)/len(dataset)*100:.1f}%)\")\n",
    "print(f\"  Late game (9+):        {sum(1 for m in move_counts if m >= 9)} ({sum(1 for m in move_counts if m >= 9)/len(dataset)*100:.1f}%)\")\n",
    "print(f\"  Avg moves per state:   {np.mean(move_counts):.1f}\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample training prompt (first 300 chars):\")\n",
    "print(dataset[0][\"prompt\"][0][\"content\"][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4e4491",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Configure GRPO Training\n",
    "\n",
    "Set up GRPO trainer with all hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "478959ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 8\n",
      "Training configuration:\n",
      "  Model:               Qwen2.5-14B-Instruct\n",
      "  Max steps:           500\n",
      "  Generations/state:   8\n",
      "  Learning rate:       2e-05\n",
      "  LR scheduler:       SchedulerType.COSINE\n",
      "  Max grad norm:       0.5\n",
      "  Loss type:           bnpo\n",
      "  Beta (KL penalty):   0.001\n",
      "  Reward weights:      [0.15, 0.7, 0.15]\n",
      "  Prompt/Completion:   700/200\n",
      "  Temperature:         0.9\n",
      "  Top-p:               0.95\n",
      "  LoRA rank:           32\n"
     ]
    }
   ],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# ── Lengths ──\n",
    "max_prompt_length = 700    # Increased: prompts now include logic hints\n",
    "max_completion_length = 200  # Short JSON output — no reasoning text\n",
    "\n",
    "# ── GRPO Configuration (research-backed) ──\n",
    "# Sources: DeepSeekMath paper, TRL docs, Open-R1 blog, DAPO paper\n",
    "training_args = GRPOConfig(\n",
    "    # === Generation ===\n",
    "    temperature = 0.9,           # Exploration during training\n",
    "    top_p = 0.95,\n",
    "\n",
    "    # === Optimization ===\n",
    "    learning_rate = 2e-5,        # Lower LR → more stable RL training\n",
    "    weight_decay = 0.01,\n",
    "    warmup_ratio = 0.05,         # Shorter warmup for RL\n",
    "    lr_scheduler_type = \"cosine\",  # Cosine > linear (Loshchilov & Hutter 2017)\n",
    "    optim = \"adamw_8bit\",        # 8-bit Adam saves VRAM (Open-R1 lesson 5)\n",
    "    max_grad_norm = 0.5,         # Tighter gradient clipping for stability\n",
    "\n",
    "    # === Batch sizes ===\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    num_generations = 8,         # More generations → better reward estimation\n",
    "\n",
    "    # === Lengths ===\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_completion_length,\n",
    "\n",
    "    # === Training duration ===\n",
    "    max_steps = 500,             # Adjust based on compute budget\n",
    "    save_steps = 100,\n",
    "\n",
    "    # === GRPO specific ===\n",
    "    # beta=0.0 (default) — KL term excluded per Open-Reasoner-Zero findings\n",
    "    # loss_type=\"dapo\" (default) — eliminates length bias (DAPO paper)\n",
    "    # scale_rewards=\"group\" (default) — normalize within group\n",
    "    num_iterations = 1,          # Standard single-iteration GRPO\n",
    "\n",
    "    # === Reward weighting (gameplay >> format >> strategy) ===\n",
    "    reward_weights = [0.15, 0.70, 0.15],\n",
    "\n",
    "    # === Output ===\n",
    "    report_to = \"none\",\n",
    "    output_dir = \"minesweeper_grpo_v2\",\n",
    "    seed = 42,\n",
    "    bf16 = True,\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Model:               Qwen2.5-14B-Instruct\")\n",
    "print(f\"  Max steps:           {training_args.max_steps}\")\n",
    "print(f\"  Generations/state:   {training_args.num_generations}\")\n",
    "print(f\"  Learning rate:       {training_args.learning_rate}\")\n",
    "print(f\"  LR scheduler:       {training_args.lr_scheduler_type}\")\n",
    "print(f\"  Max grad norm:       {training_args.max_grad_norm}\")\n",
    "print(f\"  Loss type:           {training_args.loss_type}\")\n",
    "print(f\"  Beta (KL penalty):   {training_args.beta}\")\n",
    "print(f\"  Reward weights:      {training_args.reward_weights}\")\n",
    "print(f\"  Prompt/Completion:   {max_prompt_length}/{max_completion_length}\")\n",
    "print(f\"  Temperature:         {training_args.temperature}\")\n",
    "print(f\"  Top-p:               {training_args.top_p}\")\n",
    "print(f\"  LoRA rank:           {lora_rank}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b25da67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval callback: 10 games every 50 steps (temp=0.3 for deterministic eval)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class MinesweeperEvalCallback(TrainerCallback):\n",
    "    \"\"\"Periodically play games during training and log win rate + metrics.\"\"\"\n",
    "\n",
    "    def __init__(self, eval_every_steps=50, num_games=10):\n",
    "        self.eval_every_steps = eval_every_steps\n",
    "        self.num_games = num_games\n",
    "\n",
    "    def on_step_end(self, args, state, control, model=None, processing_class=None, **kwargs):\n",
    "        if state.global_step % self.eval_every_steps != 0:\n",
    "            return\n",
    "\n",
    "        tokenizer = processing_class\n",
    "        if tokenizer is None or model is None:\n",
    "            return\n",
    "\n",
    "        was_training = model.training\n",
    "        model.eval()\n",
    "\n",
    "        wins = 0\n",
    "        total_moves = 0\n",
    "        invalid_count = 0\n",
    "        for i in range(self.num_games):\n",
    "            game = MinesweeperGame(rows=6, cols=6, num_mines=5, seed=10000 + i)\n",
    "            moves = 0\n",
    "            invalids = 0\n",
    "            while game.state() == \"ongoing\" and moves < 50:\n",
    "                prompt = format_state_for_llm(game)\n",
    "                text = tokenizer.apply_chat_template(\n",
    "                    [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True,\n",
    "                )\n",
    "                with torch.no_grad():\n",
    "                    output = model.generate(\n",
    "                        **tokenizer(text, return_tensors=\"pt\").to(model.device),\n",
    "                        temperature=0.3,       # Low temp for eval (deterministic)\n",
    "                        max_new_tokens=128,\n",
    "                        do_sample=True,\n",
    "                        top_p=0.8,\n",
    "                    )\n",
    "                response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "                action = parse_llm_action(response)\n",
    "                if action is None:\n",
    "                    invalids += 1\n",
    "                    if invalids >= 3:\n",
    "                        break\n",
    "                    continue\n",
    "                invalids = 0\n",
    "                game.do_action(action)\n",
    "                moves += 1\n",
    "            if game.state() == \"success\":\n",
    "                wins += 1\n",
    "            total_moves += moves\n",
    "            invalid_count += invalids\n",
    "\n",
    "        win_rate = wins / self.num_games\n",
    "        avg_moves = total_moves / self.num_games\n",
    "        print(f\"\\n[Eval @ step {state.global_step}] \"\n",
    "              f\"Win: {wins}/{self.num_games} ({win_rate*100:.0f}%) | \"\n",
    "              f\"Avg moves: {avg_moves:.1f} | \"\n",
    "              f\"Invalid outputs: {invalid_count}\\n\")\n",
    "\n",
    "        if was_training:\n",
    "            model.train()\n",
    "\n",
    "eval_callback = MinesweeperEvalCallback(eval_every_steps=50, num_games=10)\n",
    "print(\"Eval callback: 10 games every 50 steps (temp=0.3 for deterministic eval)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb8e106",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Train the Model\n",
    "\n",
    "Start GRPO training with reward functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d03871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GRPO training with 3 reward functions...\n",
      "  [1] valid_json_reward  (weight: 0.15)\n",
      "  [2] gameplay_scores    (weight: 0.70)\n",
      "  [3] strategic_reward   (weight: 0.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,000 | Num Epochs = 1 | Total steps = 500\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 137,625,600 of 14,907,659,264 (0.92% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  9/500 00:38 < 45:13, 0.18 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completions / mean_length</th>\n",
       "      <th>completions / min_length</th>\n",
       "      <th>completions / max_length</th>\n",
       "      <th>completions / clipped_ratio</th>\n",
       "      <th>completions / mean_terminated_length</th>\n",
       "      <th>completions / min_terminated_length</th>\n",
       "      <th>completions / max_terminated_length</th>\n",
       "      <th>sampling / sampling_logp_difference / mean</th>\n",
       "      <th>sampling / sampling_logp_difference / max</th>\n",
       "      <th>sampling / importance_sampling_ratio / min</th>\n",
       "      <th>sampling / importance_sampling_ratio / mean</th>\n",
       "      <th>sampling / importance_sampling_ratio / max</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / valid_json_reward / mean</th>\n",
       "      <th>rewards / valid_json_reward / std</th>\n",
       "      <th>rewards / gameplay_scores / mean</th>\n",
       "      <th>rewards / gameplay_scores / std</th>\n",
       "      <th>rewards / strategic_reward / mean</th>\n",
       "      <th>rewards / strategic_reward / std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.775000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>4.310527</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.524001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.437500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>18.008959</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.813925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-3.412500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.625000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.625000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.023024</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>19.423962</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.813925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.428125</td>\n",
       "      <td>1.177424</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.468750</td>\n",
       "      <td>16.329241</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.155264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.621875</td>\n",
       "      <td>2.184623</td>\n",
       "      <td>14.625000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.625000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.028749</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>11.830754</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.016001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.565625</td>\n",
       "      <td>2.722260</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.343750</td>\n",
       "      <td>16.736303</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.540002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.637500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>4.158163</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>1.319824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        valid_json_reward,   # Reward valid JSON format + conciseness\n",
    "        gameplay_scores,     # Core gameplay (all 12 criteria)\n",
    "        strategic_reward,    # Logical deduction bonuses\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    "    callbacks = [eval_callback],  # Periodic gameplay evaluation\n",
    ")\n",
    "\n",
    "print(\"Starting GRPO training with 3 reward functions...\")\n",
    "print(\"  [1] valid_json_reward  (weight: 0.15)\")\n",
    "print(\"  [2] gameplay_scores    (weight: 0.70)\")\n",
    "print(\"  [3] strategic_reward   (weight: 0.15)\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf8d025",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Test Trained Model\n",
    "\n",
    "Evaluate the finetuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00276c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on new game\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "test_game = MinesweeperGame(rows=6, cols=6, num_mines=5, seed=99)\n",
    "test_prompt = format_state_for_llm(test_game)\n",
    "\n",
    "test_text = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": test_prompt}],\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True,\n",
    ")\n",
    "\n",
    "print(\"=== Trained Model Response ===\")\n",
    "output = model.generate(\n",
    "    **tokenizer(test_text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    temperature = 0.3,\n",
    "    max_new_tokens = 128,\n",
    "    do_sample = True,\n",
    "    top_p = 0.8,\n",
    "    repetition_penalty = 1.2,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")\n",
    "\n",
    "# Parse and test action\n",
    "response_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "action = parse_llm_action(response_text)\n",
    "print(f\"\\nParsed action: {action}\")\n",
    "\n",
    "if action:\n",
    "    result = test_game.do_action(action)\n",
    "    print(f\"Action result: {result}\")\n",
    "    print(f\"Game state: {test_game.state()}\")\n",
    "    print(test_game.pretty_print())\n",
    "else:\n",
    "    print(\"⚠️ Failed to parse a valid action from the response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d2551f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Evaluation: Play Complete Games\n",
    "\n",
    "Test the model on multiple complete games:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31315fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_full_game(model, tokenizer, rows=6, cols=6, num_mines=5, seed=None,\n",
    "                   max_moves=50, verbose=False):\n",
    "    \"\"\"Play a complete Minesweeper game with the model, tracking detailed metrics.\"\"\"\n",
    "    game = MinesweeperGame(rows=rows, cols=cols, num_mines=num_mines, seed=seed)\n",
    "    moves = 0\n",
    "    invalid_moves = 0\n",
    "    logical_moves = 0\n",
    "    flags_correct = 0\n",
    "    flags_wrong = 0\n",
    "\n",
    "    while game.state() == \"ongoing\" and moves < max_moves:\n",
    "        prompt = format_state_for_llm(game)\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": prompt}],\n",
    "            tokenize = False,\n",
    "            add_generation_prompt = True,\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n",
    "                temperature = 0.3,       # Low temp for deterministic eval\n",
    "                max_new_tokens = 128,\n",
    "                do_sample = True,\n",
    "                top_p = 0.8,\n",
    "                repetition_penalty = 1.2,\n",
    "            )\n",
    "\n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        action = parse_llm_action(response)\n",
    "\n",
    "        if action is None:\n",
    "            invalid_moves += 1\n",
    "            if invalid_moves >= 3:\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        invalid_moves = 0  # Reset streak on valid move\n",
    "\n",
    "        # Track logical moves\n",
    "        safe = _compute_safe_cells(game)\n",
    "        mine = _compute_mine_cells(game)\n",
    "        if action[\"type\"] == \"reveal\" and [action[\"row\"], action[\"col\"]] in safe:\n",
    "            logical_moves += 1\n",
    "        elif action[\"type\"] == \"flag\" and [action[\"row\"], action[\"col\"]] in mine:\n",
    "            logical_moves += 1\n",
    "\n",
    "        # Track flag accuracy\n",
    "        if action[\"type\"] == \"flag\":\n",
    "            r, c = action[\"row\"], action[\"col\"]\n",
    "            if 0 <= r < game.rows and 0 <= c < game.cols:\n",
    "                if game._board[r][c] == -1:\n",
    "                    flags_correct += 1\n",
    "                else:\n",
    "                    flags_wrong += 1\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"  Move {moves}: {action}\")\n",
    "\n",
    "        game.do_action(action)\n",
    "        moves += 1\n",
    "\n",
    "    return {\n",
    "        \"game\": game,\n",
    "        \"moves\": moves,\n",
    "        \"logical_moves\": logical_moves,\n",
    "        \"flags_correct\": flags_correct,\n",
    "        \"flags_wrong\": flags_wrong,\n",
    "        \"result\": game.state(),\n",
    "    }\n",
    "\n",
    "\n",
    "# ── Comprehensive Evaluation ──\n",
    "NUM_EVAL_GAMES = 100\n",
    "print(f\"Evaluating model on {NUM_EVAL_GAMES} games...\\n\")\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # Enable Unsloth fast inference\n",
    "\n",
    "wins = 0\n",
    "total_moves = 0\n",
    "total_logical = 0\n",
    "total_flags_correct = 0\n",
    "total_flags_wrong = 0\n",
    "results_counter = {\"success\": 0, \"failed\": 0, \"ongoing\": 0}\n",
    "\n",
    "for i in range(NUM_EVAL_GAMES):\n",
    "    info = play_full_game(model, tokenizer, seed=i + 5000)\n",
    "    result = info[\"result\"]\n",
    "    results_counter[result] = results_counter.get(result, 0) + 1\n",
    "\n",
    "    if result == \"success\":\n",
    "        wins += 1\n",
    "    if i < 10 or result == \"success\":\n",
    "        cells_revealed = len(info[\"game\"]._revealed)\n",
    "        total_safe = info[\"game\"].rows * info[\"game\"].cols - info[\"game\"].num_mines\n",
    "        tag = \"🏆 WIN\" if result == \"success\" else \"💀 LOSS\" if result == \"failed\" else \"⏱ TIMEOUT\"\n",
    "        print(f\"Game {i+1:3d}: {tag} | {info['moves']:2d} moves | \"\n",
    "              f\"{info['logical_moves']} logical | \"\n",
    "              f\"{cells_revealed}/{total_safe} revealed | \"\n",
    "              f\"flags ✓{info['flags_correct']} ✗{info['flags_wrong']}\")\n",
    "\n",
    "    total_moves += info[\"moves\"]\n",
    "    total_logical += info[\"logical_moves\"]\n",
    "    total_flags_correct += info[\"flags_correct\"]\n",
    "    total_flags_wrong += info[\"flags_wrong\"]\n",
    "\n",
    "if NUM_EVAL_GAMES > 10:\n",
    "    print(f\"... (first 10 + wins shown; {NUM_EVAL_GAMES} total)\")\n",
    "\n",
    "print(f\"\\n{'='*55}\")\n",
    "print(f\"  RESULTS ({NUM_EVAL_GAMES} games)\")\n",
    "print(f\"{'='*55}\")\n",
    "print(f\"  Win rate:            {wins}/{NUM_EVAL_GAMES} ({wins/NUM_EVAL_GAMES*100:.1f}%)\")\n",
    "print(f\"  Avg moves/game:      {total_moves/NUM_EVAL_GAMES:.1f}\")\n",
    "print(f\"  Avg logical moves:   {total_logical/NUM_EVAL_GAMES:.1f}\")\n",
    "print(f\"  Flag accuracy:       {total_flags_correct}/{total_flags_correct+total_flags_wrong} \"\n",
    "      f\"({total_flags_correct/(total_flags_correct+total_flags_wrong)*100:.1f}%)\" if total_flags_correct + total_flags_wrong > 0 else \"  Flags: none placed\")\n",
    "print(f\"  Outcomes:            {results_counter}\")\n",
    "print(f\"{'='*55}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7306cc45",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Save the Model\n",
    "\n",
    "Save your trained model for competition submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b979b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters\n",
    "model.save_pretrained(\"my_minesweeper_model\")\n",
    "tokenizer.save_pretrained(\"my_minesweeper_model\")\n",
    "print(\"✅ LoRA adapters saved to: my_minesweeper_model/\")\n",
    "\n",
    "# Save merged model in 16bit (local file name which will be used for eval)\n",
    "if True:\n",
    "    model.save_pretrained_merged(\n",
    "        \"my_minesweeper_model_merged\",\n",
    "        tokenizer,\n",
    "        save_method = \"merged_16bit\"\n",
    "    )\n",
    "    print(\"✅ Merged 16-bit model saved to: my_minesweeper_model_merged/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c190f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Improvements Applied (Research-Backed)\n",
    "\n",
    "| # | Change | Source |\n",
    "|---|--------|--------|\n",
    "| 1 | Model: Qwen2.5-14B-Instruct (14B params, stronger reasoning) | Larger model for better Minesweeper logic |\n",
    "| 2 | Loaded from /workspace/Qwen2.5-14B-Instruct | Local cached model path |\n",
    "| 3 | LoRA rank 16→32, alpha=rank, dropout=0.05 | LoRA best practices |\n",
    "| 4 | Prompt with pre-computed logical hints | Chain-of-thought (Wei 2022) |\n",
    "| 5 | Complete 12-criterion `gameplay_scores` | Competition scoring rubric |\n",
    "| 6 | `strategic_reward` — bonus for deducible moves | Reward shaping (Ng 1999) |\n",
    "| 7 | `valid_json_reward` — conciseness + format | InstructGPT (2022) |\n",
    "| 8 | Curriculum dataset: fresh→early→mid→late | Bengio et al. (2009) |\n",
    "| 9 | 2000 training samples (up from 1000) | More diversity |\n",
    "| 10 | `num_generations=8` (up from 4) | Better reward estimation |\n",
    "| 11 | Cosine LR schedule + LR 2e-5 | Loshchilov & Hutter (2017) |\n",
    "| 12 | `loss_type=\"dapo\"` (default) | DAPO paper — eliminates length bias |\n",
    "| 13 | `reward_weights=[0.15, 0.70, 0.15]` | TRL reward weighting |\n",
    "| 14 | Low temperature (0.3) at evaluation | Deterministic eval |\n",
    "| 15 | `FastLanguageModel.for_inference()` | Unsloth fast inference |\n",
    "| 16 | `max_seq_length=2048` | Sufficient for Minesweeper |\n",
    "\n",
    "## Further Tuning Ideas\n",
    "- Increase `max_steps` to 1000+ for longer training\n",
    "- Try `loss_type=\"dr_grpo\"` (Dr. GRPO paper) to further reduce bias\n",
    "- Set `scale_rewards=\"batch\"` (PPO Lite paper) for batch-level normalization\n",
    "- Add `mask_truncated_completions=True` for training stability (DAPO)\n",
    "- Try `num_iterations=2` for generation reuse (speeds up training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46952a1e-915d-4b2f-872a-2ad2d70a7155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09751230-51dc-4997-8b7b-3c87e9773895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45016553-eaa9-4a9e-9e8b-dec9ee50b37c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e08ad44-0c9a-498e-8b3f-7629d447b6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b786fbd-c069-454d-a1f9-7625b7bd87e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f393470-7ac1-4ce4-9508-d3a91fbe43ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8e729a-7064-4928-a991-7da14ea4d087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b591455-067f-42db-bff0-5ccf0eef3ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42152a4-09c5-433b-beb7-21e406915e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dc480f-a720-46cc-870a-d0175d97711a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
